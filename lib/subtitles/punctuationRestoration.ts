import * as ort from 'onnxruntime-web/wasm'
import { BPETokenizer } from './bpeTokenizer'

export interface TimedToken {
  start: number // å¼€å§‹æ—¶é—´ï¼ˆç§’ï¼‰
  end: number // ç»“æŸæ—¶é—´ï¼ˆç§’ï¼‰
  text: string // åŸå§‹æ–‡æœ¬ï¼ˆå°å†™ï¼Œæ— æ ‡ç‚¹ï¼‰
}

export interface AnnotatedToken extends TimedToken {
  casedText: string // è½¬æ¢å¤§å°å†™åçš„æ–‡æœ¬
  punctuation: string // è¦æ·»åŠ çš„æ ‡ç‚¹ç¬¦å·
  caseType: 'LOWER' | 'UPPER' | 'CAP' | 'MIX'
  punctType: 'NONE' | 'COMMA' | 'PERIOD' | 'QUESTION'
}

// æ ‡ç‚¹å’Œå¤§å°å†™æ˜ å°„
const PUNCT_MAP: Record<number, string> = {
  0: '', // NO_PUNCT
  1: ',', // COMMA
  2: '.', // PERIOD
  3: '?', // QUESTION
}

const CASE_TYPE_MAP = ['LOWER', 'UPPER', 'CAP', 'MIX'] as const
const PUNCT_TYPE_MAP = ['NONE', 'COMMA', 'PERIOD', 'QUESTION'] as const

const CASE_MAP: Record<number, (word: string) => string> = {
  0: (w) => w.toLowerCase(), // LOWER
  1: (w) => w.toUpperCase(), // UPPER
  2: (w) => w.charAt(0).toUpperCase() + w.slice(1).toLowerCase(), // CAP
  3: (w) => w, // MIX_CASE
}

export class PunctuationRestorationModel {
  private session: ort.InferenceSession | null = null
  private maxSeqLength = 200
  private maxTokensPerWindow = 180 // ä¸º <s> </s> ç­‰ç•™å‡ºç©ºé—´
  private overlap = 30 // token çº§åˆ«çš„é‡å 

  private tokenizer: BPETokenizer
  constructor(tokenizer: BPETokenizer) {
    this.tokenizer = tokenizer
  }

  async load(modelPath: string, wasmUrl?: string) {
    ort.env.wasm.wasmPaths = {
      wasm: wasmUrl,
    }
    ort.env.logLevel = 'error'
    this.session = await ort.InferenceSession.create(modelPath)
  }

  /**
   * è®¾ç½®çª—å£å‚æ•°ï¼ˆåŸºäº token æ•°é‡ï¼Œè€Œéè¯æ•°ï¼‰
   */
  setWindowConfig(maxTokensPerWindow: number, overlap: number) {
    if (maxTokensPerWindow > this.maxSeqLength - 20) {
      console.warn(
        `Window size ${maxTokensPerWindow} is too large, using ${
          this.maxSeqLength - 20
        }`,
      )
      this.maxTokensPerWindow = this.maxSeqLength - 20
    } else {
      this.maxTokensPerWindow = maxTokensPerWindow
    }
    this.overlap = Math.min(overlap, maxTokensPerWindow / 3)
  }

  /**
   * ä½¿ç”¨åŒæŒ‡é’ˆåŒ¹é…å°† BPE tokens æ˜ å°„å›åŸå§‹å•è¯
   */
  private matchBPEToWords(
    words: string[],
    bpeTokenIds: number[],
    validIds: number[],
  ): Map<number, number[]> {
    // è¿”å›ï¼šwordIndex -> [bpeTokenIndices]
    const wordToBPE = new Map<number, number[]>()

    if (!this.tokenizer) throw new Error('Tokenizer not loaded')

    let wordIdx = 0
    let bpeIdx = 0

    // è·³è¿‡å¼€å§‹æ ‡è®° <s>
    if (bpeTokenIds[0] === this.tokenizer.pieceToId('<s>')) {
      bpeIdx = 1
    }

    while (wordIdx < words.length && bpeIdx < bpeTokenIds.length) {
      // è·³è¿‡ç»“æŸæ ‡è®°
      if (bpeTokenIds[bpeIdx] === this.tokenizer.pieceToId('</s>')) {
        break
      }

      const bpeIndices: number[] = []
      const currentWord = words[wordIdx].toLowerCase()

      // ç´¯ç§¯ BPE tokens ç›´åˆ°åŒ¹é…å½“å‰å•è¯
      let accumulatedText = ''

      while (bpeIdx < bpeTokenIds.length) {
        const bpeToken = this.tokenizer.decode([bpeTokenIds[bpeIdx]])
        accumulatedText += bpeToken

        // åªè®°å½• valid çš„ token ç´¢å¼•
        if (validIds[bpeIdx] === 1) {
          bpeIndices.push(bpeIdx)
        }

        bpeIdx++

        // æ£€æŸ¥æ˜¯å¦åŒ¹é…å½“å‰å•è¯ï¼ˆç§»é™¤ç©ºæ ¼å’Œ â– ç¬¦å·ï¼‰
        const normalized = accumulatedText.replace(/[â–\s]/g, '').toLowerCase()
        if (normalized === currentWord) {
          break
        }

        // é˜²æ­¢æ— é™å¾ªç¯
        if (bpeIdx >= bpeTokenIds.length) break
      }

      if (bpeIndices.length > 0) {
        wordToBPE.set(wordIdx, bpeIndices)
      }

      wordIdx++
    }

    return wordToBPE
  }

  /**
   * å°†é¢„æµ‹ç»“æœæ˜ å°„å›åŸå§‹ tokensï¼ˆç®€åŒ–ç‰ˆï¼šç›´æ¥ç´¢å¼•æ˜ å°„ï¼‰
   */
  private mapPredictionsToTokens(
    tokens: TimedToken[],
    _wordToBPE: Map<number, number[]>,
    casePred: number[],
    punctPred: number[],
  ): AnnotatedToken[] {
    return tokens.map((token, wordIdx) => {
      // ç›´æ¥ä½¿ç”¨è¯ç´¢å¼•è·å–é¢„æµ‹ï¼ˆæ¨¡å‹è¾“å‡ºå·²ç»å»æ‰äº† <s> å’Œ </s>ï¼‰
      // é¢„æµ‹ç»“æœçš„ç´¢å¼•ç›´æ¥å¯¹åº”è¯çš„ç´¢å¼•
      const caseType = casePred[wordIdx] !== undefined ? casePred[wordIdx] : 0
      const punctType =
        punctPred[wordIdx] !== undefined ? punctPred[wordIdx] : 0

      return {
        ...token,
        casedText: CASE_MAP[caseType](token.text),
        punctuation: PUNCT_MAP[punctType],
        caseType: CASE_TYPE_MAP[caseType],
        punctType: PUNCT_TYPE_MAP[punctType],
      }
    })
  }

  /**
   * ä¸»å‡½æ•°ï¼šä¸ºå¸¦æ—¶é—´æˆ³çš„ tokens æ·»åŠ æ ‡ç‚¹å’Œå¤§å°å†™ï¼ˆæ”¯æŒè‡ªåŠ¨åˆ†æ®µï¼‰
   */
  async annotatePunctuation(tokens: TimedToken[]): Promise<AnnotatedToken[]> {
    if (!this.session || !this.tokenizer) {
      throw new Error('Model not loaded. Call load() first.')
    }

    // å…ˆæ•´ä½“ tokenize ä¸€æ¬¡ï¼Œåˆ¤æ–­æ˜¯å¦éœ€è¦åˆ†æ®µ
    const fullText = tokens.map((t) => t.text).join(' ')
    const { tokenIds } = this.tokenizer.encode(fullText)

    // console.log(
    //   `ğŸ“Š Total tokens: ${tokens.length} words â†’ ${tokenIds.length} BPE tokens`,
    // )

    // å¦‚æœ BPE tokens æ•°é‡åœ¨é™åˆ¶å†…ï¼Œç›´æ¥å¤„ç†
    if (tokenIds.length <= this.maxTokensPerWindow) {
      return this.annotatePunctuationSingleWindow(tokens)
    }

    // å¦åˆ™ä½¿ç”¨åŸºäº token é•¿åº¦çš„æ»‘åŠ¨çª—å£
    return this.annotatePunctuationWithTokenBasedWindow(tokens)
  }

  /**
   * åŸºäº BPE token é•¿åº¦çš„æ™ºèƒ½åˆ†çª—å£
   */
  private async annotatePunctuationWithTokenBasedWindow(
    tokens: TimedToken[],
  ): Promise<AnnotatedToken[]> {
    if (!this.tokenizer) throw new Error('Tokenizer not loaded')

    // console.log(
    //   `ğŸªŸ Using token-based sliding window (max=${this.maxTokensPerWindow} tokens, overlap=${this.overlap})`,
    // )

    const results: AnnotatedToken[] = []
    let wordStart = 0
    let windowCount = 0

    while (wordStart < tokens.length) {
      windowCount++

      // ä»å½“å‰ä½ç½®å¼€å§‹ï¼Œç´¯ç§¯è¯ç›´åˆ°è¾¾åˆ° token é™åˆ¶
      let wordEnd = wordStart
      let currentTokenCount = 0

      while (
        wordEnd < tokens.length &&
        currentTokenCount < this.maxTokensPerWindow
      ) {
        // è¯•æ¢æ€§ tokenize
        const testText = tokens
          .slice(wordStart, wordEnd + 1)
          .map((t) => t.text)
          .join(' ')
        const { tokenIds } = this.tokenizer.encode(testText)

        if (tokenIds.length > this.maxTokensPerWindow) {
          // è¶…è¿‡é™åˆ¶ï¼Œå›é€€ä¸€ä¸ªè¯
          break
        }

        currentTokenCount = tokenIds.length
        wordEnd++
      }

      // ç¡®ä¿è‡³å°‘å¤„ç†ä¸€äº›è¯
      if (wordEnd === wordStart) {
        wordEnd = wordStart + 1
      }

      const windowTokens = tokens.slice(wordStart, wordEnd)

      // console.log(
      //   `ğŸªŸ Window ${windowCount}: words ${wordStart}-${wordEnd} (${windowTokens.length} words, ~${currentTokenCount} tokens)`,
      // )

      // å¤„ç†å½“å‰çª—å£
      const windowResults = await this.annotatePunctuationSingleWindow(
        windowTokens,
      )

      // å¤„ç†é‡å ï¼šåªä¿ç•™éé‡å éƒ¨åˆ†
      const overlapWords = Math.floor(this.overlap / 2) // ä¼°ç®—è¯çº§åˆ«çš„é‡å 
      const keepStart = wordStart === 0 ? 0 : overlapWords
      const keepEnd =
        wordEnd >= tokens.length
          ? windowResults.length
          : windowResults.length - overlapWords

      for (let i = keepStart; i < keepEnd; i++) {
        if (i < windowResults.length) {
          results.push(windowResults[i])
        }
      }

      // ç§»åŠ¨çª—å£ï¼ˆè€ƒè™‘é‡å ï¼‰
      wordStart = wordEnd - overlapWords * 2

      // é˜²æ­¢æ— é™å¾ªç¯
      if (wordStart <= wordEnd - windowTokens.length) {
        wordStart = wordEnd
      }

      await new Promise((resolve) => setTimeout(resolve, 0)) // è®©å‡ºäº‹ä»¶å¾ªç¯
    }

    // console.log(
    //   `âœ… Processed ${results.length} tokens in ${windowCount} windows`,
    // )

    return results
  }

  /**
   * å¤„ç†å•ä¸ªçª—å£ï¼ˆåŸæœ‰é€»è¾‘ï¼‰
   */
  private async annotatePunctuationSingleWindow(
    tokens: TimedToken[],
  ): Promise<AnnotatedToken[]> {
    if (!this.session || !this.tokenizer) {
      throw new Error('Model not loaded. Call load() first.')
    }

    // 1. æå–æ–‡æœ¬å¹¶ tokenize
    const text = tokens.map((t) => t.text).join(' ')
    const words = tokens.map((t) => t.text)

    const { tokenIds, wordBoundaries } = this.tokenizer.encode(text)

    // 2. åˆ›å»º valid_ids (åªæ ‡è®°è¯è¾¹ç•Œä½ç½®)
    const validIds = new Array(this.maxSeqLength).fill(0)

    // æ ‡è®°è¯è¾¹ç•Œä½ç½®ä¸º valid
    for (const boundary of wordBoundaries) {
      if (boundary < this.maxSeqLength) {
        validIds[boundary] = 1
      }
    }

    // const validCount = wordBoundaries.length

    // console.log('Tokenization info:', {
    //   text,
    //   words,
    //   tokenIds: tokenIds.slice(0, 20),
    //   tokenIdsLength: tokenIds.length,
    //   wordBoundaries,
    //   validIds: validIds.slice(0, 20),
    //   validCount,
    // })

    // 3. æˆªæ–­åˆ°æœ€å¤§åºåˆ—é•¿åº¦ï¼ˆé‡è¦ï¼ï¼‰
    const truncatedTokenIds = tokenIds.slice(0, this.maxSeqLength)
    const truncatedValidIds = validIds.slice(0, this.maxSeqLength)

    // é‡æ–°è®¡ç®— validCount
    const actualValidCount = truncatedValidIds.filter((v) => v === 1).length

    // 4. Padding
    const paddedTokenIds = [...truncatedTokenIds]
    while (paddedTokenIds.length < this.maxSeqLength) {
      paddedTokenIds.push(0)
    }

    const paddedValidIds = [...truncatedValidIds]
    while (paddedValidIds.length < this.maxSeqLength) {
      paddedValidIds.push(0)
    }

    // 5. å‡†å¤‡æ¨¡å‹è¾“å…¥
    const inputTokenIds = new ort.Tensor(
      'int32',
      new Int32Array(paddedTokenIds),
      [1, this.maxSeqLength],
    )
    const inputValidIds = new ort.Tensor(
      'int32',
      new Int32Array(paddedValidIds),
      [1, this.maxSeqLength],
    )
    const labelLens = new ort.Tensor(
      'int32',
      new Int32Array([actualValidCount]),
      [1],
    )

    // 6. è¿è¡Œæ¨¡å‹
    const feeds: Record<string, ort.Tensor> = {}
    feeds[this.session.inputNames[0]] = inputTokenIds
    feeds[this.session.inputNames[1]] = inputValidIds
    feeds[this.session.inputNames[2]] = labelLens

    const results = await this.session.run(feeds)

    // 7. è§£æè¾“å‡º
    const outputNames = this.session.outputNames
    const caseLogits = results[outputNames[0]].data as Float32Array
    const punctLogits = results[outputNames[1]].data as Float32Array

    const caseClasses = 4
    const punctClasses = 4
    const numPredictions = caseLogits.length / caseClasses

    // 8. è·å–é¢„æµ‹ç»“æœï¼ˆargmaxï¼‰
    const casePred: number[] = []
    const punctPred: number[] = []

    for (let i = 0; i < numPredictions; i++) {
      let maxCaseIdx = 0
      let maxCaseVal = caseLogits[i * caseClasses]
      for (let j = 1; j < caseClasses; j++) {
        if (caseLogits[i * caseClasses + j] > maxCaseVal) {
          maxCaseVal = caseLogits[i * caseClasses + j]
          maxCaseIdx = j
        }
      }
      casePred.push(maxCaseIdx)

      let maxPunctIdx = 0
      let maxPunctVal = punctLogits[i * punctClasses]
      for (let j = 1; j < punctClasses; j++) {
        if (punctLogits[i * punctClasses + j] > maxPunctVal) {
          maxPunctVal = punctLogits[i * punctClasses + j]
          maxPunctIdx = j
        }
      }
      punctPred.push(maxPunctIdx)
    }

    // 9. åŒæŒ‡é’ˆåŒ¹é…ï¼šBPE tokens -> åŸå§‹å•è¯
    const wordToBPE = this.matchBPEToWords(words, paddedTokenIds, validIds)

    // console.log('Model predictions:', {
    //   numPredictions,
    //   wordCount: words.length,
    //   validCount: actualValidCount,
    //   tokenIdsLength: tokenIds.length,
    //   truncated: tokenIds.length > this.maxSeqLength,
    //   casePred: casePred.slice(0, 15),
    //   punctPred: punctPred.slice(0, 15),
    //   caseMapping: words.slice(0, 10).map((w, i) => ({
    //     word: w,
    //     caseType: CASE_TYPE_MAP[casePred[i]] || 'UNDEFINED',
    //     punctType: PUNCT_TYPE_MAP[punctPred[i]] || 'UNDEFINED',
    //   })),
    //   caseLogitsSize: caseLogits.length,
    //   punctLogitsSize: punctLogits.length,
    // })

    if (tokenIds.length > this.maxSeqLength) {
      console.warn(
        `âš ï¸ Input text is too long! Truncated from ${tokenIds.length} to ${this.maxSeqLength} tokens. Some words may not be processed.`,
      )
    }

    // 10. æ˜ å°„é¢„æµ‹ç»“æœåˆ°åŸå§‹ tokensï¼ˆç®€åŒ–ï¼šç›´æ¥ç”¨ç´¢å¼•ï¼‰
    return this.mapPredictionsToTokens(tokens, wordToBPE, casePred, punctPred)
  }

  /**
   * è¾…åŠ©å‡½æ•°ï¼šå°†æ ‡æ³¨ç»“æœæ¸²æŸ“ä¸ºå¸¦æ ‡ç‚¹çš„æ–‡æœ¬
   */
  renderAnnotatedTokens(tokens: AnnotatedToken[]): string {
    return tokens.map((t) => t.casedText + t.punctuation).join(' ')
  }
}
